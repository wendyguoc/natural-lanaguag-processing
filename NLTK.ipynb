{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'the', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and the city is awesome.\\n\n",
    "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\n",
    "tokenized_text=word_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 26 samples and 31 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist(tokenized_text)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 3), (',', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#frequency distribution plot\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m', 'they', \"it's\", 'same', 'while', \"you're\", 'into', \"wouldn't\", 'in', 'o', 'is', 'he', 'at', \"she's\", 'yours', 'both', 'a', \"hadn't\", \"hasn't\", 'me', 'few', 'we', 'under', 'haven', 'am', 't', 'hadn', \"shan't\", 'ma', 'most', 'were', 'which', 'hers', 'its', 'isn', 'are', 'or', 'over', 'now', 'through', 'it', \"weren't\", 'wouldn', 'but', 'aren', \"couldn't\", 'out', 'your', 'those', 'how', \"should've\", 'after', 'can', 'has', 'him', 'wasn', 'ourselves', 'ours', 'have', 'such', 'ain', 'yourself', \"you'll\", 'there', 'when', 'all', 'yourselves', 'more', 'weren', 'these', 'had', 'once', 'theirs', 'needn', 'then', 've', \"wasn't\", 'as', 'by', 'on', 'than', 'will', 'did', 'shouldn', 'before', 'some', 'doing', 'not', 'them', 'themselves', 's', 'so', 'why', 'because', 'with', 'about', 'above', 'myself', 'between', 'to', 'what', \"doesn't\", 'just', 'off', \"isn't\", 'own', 'no', 'only', 'of', 'any', \"didn't\", 're', 'this', 'from', 'further', 'does', 'each', 'd', \"you'd\", 'their', 'doesn', 'y', 'mustn', 'won', \"you've\", \"don't\", 'her', 'i', \"that'll\", \"mightn't\", 'she', 'that', 'who', \"mustn't\", 'during', 'where', 'you', 'being', 'too', 'very', \"needn't\", 'having', 'didn', \"won't\", 'whom', 'should', 'his', 'again', 'was', 'other', \"aren't\", 'll', 'herself', 'be', 'if', \"haven't\", 'an', 'himself', 'mightn', 'against', 'been', 'my', 'down', 'and', 'for', 'until', 'hasn', 'itself', 'couldn', 'below', 'our', 'here', 'do', 'shan', 'the', 'up', 'nor', 'don', \"shouldn't\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words:  ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords\n",
    "filtered_word=[]\n",
    "for w in tokenized_text:\n",
    "    if w not in stop_words:\n",
    "        filtered_word.append(w)\n",
    "print(\"Tokenized words: \", filtered_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "example_words=[\"python\",\"pythoning\",\"pythoned\",\"pythoner\",\"pythonly\"]\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n"
     ]
    }
   ],
   "source": [
    "new_text=\"It is very important to be pythonly while you are pythoning with python. All pythoner have pythoned poorly at least once\"\n",
    "\n",
    "words=word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c1a3ea8b18cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mnameEnt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mnameEnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m         \u001b[0mdraw_trees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\draw\\tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[1;34m(*trees)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m     \"\"\"\n\u001b[1;32m-> 1008\u001b[1;33m     \u001b[0mTreeView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\draw\\tree.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0min_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \u001b[1;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1283\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1284\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[1;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text=state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text=state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "customer_sent_tokenizer=PunktSentenceTokenizer(train_text)\n",
    "tokenized=customer_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "for i in tokenized:\n",
    "    words=word_tokenize(i)\n",
    "    tagged=nltk.pos_tag(words)\n",
    "    \n",
    "    nameEnt=nltk.ne_chunk(tagged)\n",
    "    nameEnt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\", pos='a'))\n",
    "print(lemmatizer.lemmatize(\"ran\", pos='v'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns=wordnet.synsets(\"program\")\n",
    "print(syns[0].name())\n",
    "print(syns[0].definition())\n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " L:  Lemma('good.n.01.good')\n",
      " L:  Lemma('good.n.02.good')\n",
      " L:  Lemma('good.n.02.goodness')\n",
      " L:  Lemma('good.n.03.good')\n",
      " L:  Lemma('good.n.03.goodness')\n",
      " L:  Lemma('commodity.n.01.commodity')\n",
      " L:  Lemma('commodity.n.01.trade_good')\n",
      " L:  Lemma('commodity.n.01.good')\n",
      " L:  Lemma('good.a.01.good')\n",
      " L:  Lemma('full.s.06.full')\n",
      " L:  Lemma('full.s.06.good')\n",
      " L:  Lemma('good.a.03.good')\n",
      " L:  Lemma('estimable.s.02.estimable')\n",
      " L:  Lemma('estimable.s.02.good')\n",
      " L:  Lemma('estimable.s.02.honorable')\n",
      " L:  Lemma('estimable.s.02.respectable')\n",
      " L:  Lemma('beneficial.s.01.beneficial')\n",
      " L:  Lemma('beneficial.s.01.good')\n",
      " L:  Lemma('good.s.06.good')\n",
      " L:  Lemma('good.s.07.good')\n",
      " L:  Lemma('good.s.07.just')\n",
      " L:  Lemma('good.s.07.upright')\n",
      " L:  Lemma('adept.s.01.adept')\n",
      " L:  Lemma('adept.s.01.expert')\n",
      " L:  Lemma('adept.s.01.good')\n",
      " L:  Lemma('adept.s.01.practiced')\n",
      " L:  Lemma('adept.s.01.proficient')\n",
      " L:  Lemma('adept.s.01.skillful')\n",
      " L:  Lemma('adept.s.01.skilful')\n",
      " L:  Lemma('good.s.09.good')\n",
      " L:  Lemma('dear.s.02.dear')\n",
      " L:  Lemma('dear.s.02.good')\n",
      " L:  Lemma('dear.s.02.near')\n",
      " L:  Lemma('dependable.s.04.dependable')\n",
      " L:  Lemma('dependable.s.04.good')\n",
      " L:  Lemma('dependable.s.04.safe')\n",
      " L:  Lemma('dependable.s.04.secure')\n",
      " L:  Lemma('good.s.12.good')\n",
      " L:  Lemma('good.s.12.right')\n",
      " L:  Lemma('good.s.12.ripe')\n",
      " L:  Lemma('good.s.13.good')\n",
      " L:  Lemma('good.s.13.well')\n",
      " L:  Lemma('effective.s.04.effective')\n",
      " L:  Lemma('effective.s.04.good')\n",
      " L:  Lemma('effective.s.04.in_effect')\n",
      " L:  Lemma('effective.s.04.in_force')\n",
      " L:  Lemma('good.s.15.good')\n",
      " L:  Lemma('good.s.16.good')\n",
      " L:  Lemma('good.s.16.serious')\n",
      " L:  Lemma('good.s.17.good')\n",
      " L:  Lemma('good.s.17.sound')\n",
      " L:  Lemma('good.s.18.good')\n",
      " L:  Lemma('good.s.18.salutary')\n",
      " L:  Lemma('good.s.19.good')\n",
      " L:  Lemma('good.s.19.honest')\n",
      " L:  Lemma('good.s.20.good')\n",
      " L:  Lemma('good.s.20.undecomposed')\n",
      " L:  Lemma('good.s.20.unspoiled')\n",
      " L:  Lemma('good.s.20.unspoilt')\n",
      " L:  Lemma('good.s.21.good')\n",
      " L:  Lemma('well.r.01.well')\n",
      " L:  Lemma('well.r.01.good')\n",
      " L:  Lemma('thoroughly.r.02.thoroughly')\n",
      " L:  Lemma('thoroughly.r.02.soundly')\n",
      " L:  Lemma('thoroughly.r.02.good')\n"
     ]
    }
   ],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        print (\" L: \", l)\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "#print(set(synonyms))\n",
    "#print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34782608695652173\n"
     ]
    }
   ],
   "source": [
    "w1=wordnet.synset(\"ship.n.01\")\n",
    "w2=wordnet.synset(\"orange.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-41-ca7b5633fd95>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-41-ca7b5633fd95>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    for category in movie_reviews.categories()\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents[(list(movie_reviews.words(fileid)), category)\n",
    "          for category in movie_reviews.categories()\n",
    "          for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' capsule : in 2176 on the planet mars police taking into custody an accused murderer face the title menace . there is a lot of fighting and not a whole lot of story otherwise . john carpenter reprises so many ideas from his previous films , especially assault on precinct 13 , that the new film comes off as his homage to himself . , 0 ( - 4 to + 4 ) . john carpenter apparently believes that action scenes in which people fight something horrible are the same as horror scenes . for a writer and director of horror films , supposedly an expert on horror , it is a very bad mistake to make . ghosts of mars is called a horror movie , but it is more just a drawn out fight between humans and a surprisingly low - powered alien menace . in addition if anybody but john carpenter had made ghosts of mars , carpenter would have grounds to sue . this film is just chock full of pieces taken from assault on precinct 13 , the thing , and prince of darkness . it is , in fact , surprising that carpenter managed to fit so many pieces of his previous work into this film in such an admittedly novel way . but that still does not make for a really good science fiction experience . ghosts of mars takes place in the year 2176 . mars has been mostly terraformed so that humans can walk on the surface without breathing gear ( which is good for the film \\' s budget ) . it is never mentioned , but the gravity on mars has been increased somehow to earth - normal , again making it easier to film . society has changed a bit by that time , but it has advanced surprisingly little . apparently the culture has changed so that women are much more in positions of control . and from carpenter \\' s view , women have really made a mess of things . society has stagnated under female control so that beyond some minor technological advances society has changed less in 175 years than we might expect it to change in ten . the basic plot of ghosts of mars has much in common with that of assault on precinct 13 except that precinct 9 ( yes , precinct 9 ) has been replaced by a somewhat tacky looking rundown martian mining colony . instead of having the criminal \" napolean \" wilson , this film has the criminal \" desolation \" williams . instead of facing hoodlums with automatic weapons the police face , well , ghosts of mars . because the ghosts are somewhat alien in nature they should behave in some alien manner , but they essentially behave as human savages , in another lapse of imagination . the story is told in flashback , flashback within flashback , and flashback within flashback within flashback . ghosts of mars takes place entirely at night and is filmed almost entirely in tones of red , yellow , and black . carpenter manages to give us a powerful opening scene , showing a mining train rushing through the martian night to the sound of music with a heavy beat . sadly what follows is not really up to the buildup . the terror he creates looks a little too much like fugitive wannabes from the rock band kiss . his idea of building suspense is having a bunch of sudden jump scenes that sucker the viewer into thinking something scary is happening and then prove to be just something boring . these are standard haunted house film shock effects that require no great talent to give the audience . somewhat newer but also unimpressive are the cgi digital decapitations in some of the fights . within a short stretch of time we have seen the release of mission to mars , red planet , and ghosts of mars . after mission to mars was panned by too many reviewers it looks better and better and better as time goes by . i rate ghosts of mars a 4 on the 0 to 10 scale and a 0 on the - 4 to + 4 scale . following the movie i showed my wife , who liked ghosts of mars moderately more than i did , carpenter \\' s classic assault on precinct 13 . her comment is that it was seeing the same film twice .', 'sentiment': 'negative'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_movie_reviews():\n",
    "\n",
    "    # movie_reviews is a sizeable corpus to import, so only load it if we have to\n",
    "    from nltk.corpus import movie_reviews\n",
    "      \n",
    "\n",
    "    raw_data = []\n",
    "\n",
    "    # NLTK's corpus is structured in an interesting way\n",
    "    # first iterate through the two categories (pos and neg)\n",
    "    for category in movie_reviews.categories():\n",
    "\n",
    "        if category == 'pos':\n",
    "            pretty_category_name = 'positive'\n",
    "        elif category == 'neg':\n",
    "            pretty_category_name = 'negative'\n",
    "\n",
    "        # each of these categories is just fileids, so grab those\n",
    "        for fileid in movie_reviews.fileids(category):\n",
    "\n",
    "            # then each review is a NLTK class where each item in that class instance is a word\n",
    "            review_words = movie_reviews.words(fileid)\n",
    "            review_text = ''\n",
    "\n",
    "            for word in review_words:\n",
    "                review_text += ' ' + word\n",
    "\n",
    "            review_dictionary = {\n",
    "                'text': review_text,\n",
    "                'sentiment': pretty_category_name\n",
    "            }\n",
    "\n",
    "            raw_data.append(review_dictionary)\n",
    "\n",
    "    return raw_data \n",
    "\n",
    "raw_data=load_movie_reviews()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" best remembered for his understated performance as dr . hannibal lecter in michael mann ' s forensics thriller , manhunter , scottish character actor brian cox brings something special to every movie he works on . usually playing a bit role in some studio schlock ( he dies halfway through the long kiss goodnight ) , he ' s only occasionally given something meaty and substantial to do . if you want to see some brilliant acting , check out his work as a dogged police inspector opposite frances mcdormand in ken loach ' s hidden agenda . cox plays the role of big john harrigan in the disturbing new indie flick l . i . e . , which lot 47 picked up at sundance when other distributors were scared to budge . big john feels the love that dares not speak its name , but he expresses it through seeking out adolescents and bringing them back to his pad . what bothered some audience members was the presentation of big john in an oddly empathetic light . he ' s an even - tempered , funny , robust old man who actually listens to the kids ' problems ( as opposed to their parents and friends , both caught up in the high - wire act of their own confused lives . ) he ' ll have sex - for - pay with them only after an elaborate courtship , charming them with temptations from the grown - up world . l . i . e . stands for long island expressway , which slices through the strip malls and middle - class homes of suburbia . filmmaker michael cuesta uses it as a ( pretty transparent ) metaphor of dangerous escape for his 15 - year old protagonist , howie ( paul franklin dano ) . in his opening voice - over , howie reveals a morbid preoccupation with death on the road , citing the l . i . e . highway deaths of filmmaker alan j . pakula , songwriter harry chapin , and his own mother on exit 52 . he ' s both fascinated and disturbed by the l . i . e . , and those feelings are projected onto big john ( who follows howie around in his bright red car , but never makes a move to force the boy to do something he doesn ' t want to do . this makes him much more complex than the usual child molesters seen in movies -- he ' s a beast , but ashamed of it . ) l . i . e . would have worked best as a half - hour short film about howie ' s ill - advised foray into big john ' s haven . there is unnecessary padding with howie ' s miserable dad ( bruce altman ) in the hot seat for a white - collar crime , degenerate youngsters who get their kicks from robbing middle - class houses , and some homoerotic shenanigans with wise - ass gary terrio ( billy kay ) , a handsome artful dodger . rather than add to the themes of suburban ennui ( not that we needed another movie on that subject ) , these awkward subplots pad out the running time to adequate feature length . concurrently , the relationship between howie and big john is evenly paced and exceptionally well acted . cox , sporting a baseball cap and a faded marine tattoo , is all bluff and bluster . dano is quiet and at first glance seems so withdrawn as to be transparent . we ' re so used to child actors whose dramatic choices are broad and obvious ( calling haley joel ! ) , it ' s surprising to see one who actually listens throughout any given scene . the restraint is admirable . but l . i . e . ' s screenplay doesn ' t always give them the best material . when howie reads big john a walt whitman poem , the moment feels a bit too precious . director michael cuesta lingers on an ecstatic reaction shot of big john , who may as well be hearing glenn gould performing bach ' s goldberg variations . it ' s too much . there are also some obvious dramatic contrivances involving big john ' s other boy toy ( walter masterson ) , jealous over the newbie . this plot thread predictably leads to violence . not content to be a haunting , observational portrait of teen alienation in a royally screwed up world ( like terry zwigoff ' s superb ghost world ) , cuesta lacks the confidence in his own work to end on an ambivalent note . it ' s typical of unimaginative cinema to wrap things up with a bullet , sparing the writers from actually having to come up with a complex , philosophical note . in this regard , l . i . e . ( and countless other indie films ) share something in common with blockbuster action films : problems are solved when the obstacle is removed . how often does real life work this way ? to extend the question : if a movie is striving for realism , do dramatic contrivances destroy the illusion ?\", 'sentiment': 'negative'}\n"
     ]
    }
   ],
   "source": [
    "print(raw_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>plot : two teen couples go to a church party ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the happy bastard ' s quick movie review damn...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>it is movies like these that make a jaded mov...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>synopsis : a mentally unstable man undergoing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0   plot : two teen couples go to a church party ...  negative\n",
       "1   the happy bastard ' s quick movie review damn...  negative\n",
       "2   it is movies like these that make a jaded mov...  negative\n",
       "3   \" quest for camelot \" is warner bros . ' firs...  negative\n",
       "4   synopsis : a mentally unstable man undergoing...  negative"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_review=pd.DataFrame.from_dict(raw_data)\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>negative</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "sentiment      \n",
       "negative   1000\n",
       "positive   1000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     plot : two teen couples go to a church party ...\n",
       "1     the happy bastard ' s quick movie review damn...\n",
       "2     it is movies like these that make a jaded mov...\n",
       "3     \" quest for camelot \" is warner bros . ' firs...\n",
       "4     synopsis : a mentally unstable man undergoing...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x=df_review['text']\n",
    "df_y=df_review.sentiment\n",
    "df_x.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(df_x, df_y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 35735)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "token=RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv=CountVectorizer(stop_words='english', tokenizer=token.tokenize)\n",
    "x_traincv=cv.fit_transform(x_train)\n",
    "x_testcv=cv.transform(x_test)\n",
    "x_traincv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8225"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb=MultinomialNB()\n",
    "mnb.fit(x_traincv, y_train)\n",
    "mnb.score(x_testcv, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
